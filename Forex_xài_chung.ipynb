{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5zFKN-Qhu1e",
    "outputId": "13135187-1efb-42ed-8e48-7512c312b036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDCHF\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# symbols = {'all': [\"EURCHF\", \"EURNZD\", \"NZDUSD\", \"AUDNZD\", \"USDJPY\", \"NZDJPY\", \"GBPJPY\", \"USDCHF\"]}\n",
    "symbols = {'all': [\"USDCHF\"]}\n",
    "\"\"\"Đoạn code này có thể chỉnh sửa lấy loại tiền tệ theo yêu cầu\"\"\"\n",
    "def get_data(symbols):\n",
    "  data_forex = {}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    data_forex[sym] = pd.read_csv(f'D:/ForexResearch/Data/{sym}.csv', index_col=0)\n",
    "    print(sym)\n",
    "  return data_forex\n",
    "\n",
    "data_forex = get_data(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycGmMYMbUUe1"
   },
   "source": [
    "# Bước lưu lại dữ liệu và load dữ liệu lên nếu cần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "blCLOfRTwUa-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from finta import TA\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "# thư viện làm trơn nhãn\n",
    "import simdkalman\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import plotly.express as px\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q2i_Z09wtgJ"
   },
   "source": [
    "# Công đoạn này thêm các technical indicators vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zf5m8YCtlVl7"
   },
   "outputs": [],
   "source": [
    "# Hàm khởi tạo các  technical indicator\n",
    "def create_indicators(ohlcv: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = ohlcv.copy()\n",
    "    ohlcv = ohlcv.apply(pd.to_numeric, errors='coerce')\n",
    "    indi, signal = bias(ohlcv)\n",
    "    data = pd.concat([data, indi], axis=1)\n",
    "    data['BIAS_signal'] = signal\n",
    "    data['VR'], data[\"VR_signal\"] = vr(ohlcv)\n",
    "    data['TRIX'], data[\"TRIX_signal\"] = trix(ohlcv)\n",
    "    data['ER'] = TA.ER(ohlcv)\n",
    "    data['EVWMA'] = TA.EVWMA(ohlcv)\n",
    "    data['VWAP'] = TA.VWAP(ohlcv)\n",
    "    data['MOM'] = TA.MOM(ohlcv)\n",
    "    data['ROC'] = TA.ROC(ohlcv)\n",
    "    data['RSI'] = TA.RSI(ohlcv)\n",
    "    data['IFT_RSI'] = TA.IFT_RSI(ohlcv)\n",
    "    data['ATR'] = TA.ATR(ohlcv)\n",
    "    data['BBWIDTH'] = TA.BBWIDTH(ohlcv)\n",
    "    data['ADX'] = TA.ADX(ohlcv)\n",
    "    data['STOCH'] = TA.STOCH(ohlcv)\n",
    "    data['STOCHD'] = TA.STOCHD(ohlcv)\n",
    "    data['AO'] = TA.AO(ohlcv)\n",
    "    data['MI'] = TA.MI(ohlcv)\n",
    "    data['MFI'] = TA.MFI(ohlcv)\n",
    "    data['PZO'] = TA.PZO(ohlcv)\n",
    "    data['EFI'] = TA.EFI(ohlcv)\n",
    "    data['EMV'] = TA.EMV(ohlcv)\n",
    "    data['CCI'] = TA.CCI(ohlcv)\n",
    "    data['FISH'] = TA.FISH(ohlcv)\n",
    "    data['FVE'] = TA.FVE(ohlcv)\n",
    "\n",
    "    macd = TA.MACD(ohlcv)\n",
    "    data['MACDCal'] = macd['MACD'] - macd['SIGNAL']\n",
    "\n",
    "    macdev = TA.EV_MACD(ohlcv)\n",
    "    data['EVMACD'] = macdev[\"MACD\"]\n",
    "\n",
    "    data['TR'] = TA.TR(ohlcv)\n",
    "\n",
    "    DMI = TA.DMI(ohlcv)\n",
    "    data['DMI+'] = DMI[\"DI+\"]\n",
    "    data['DMI-'] = DMI[\"DI-\"]\n",
    "\n",
    "    # VORTEX = TA.VORTEX(ohlcv)\n",
    "    # data['VIp'] = VORTEX[\"VIp\"]\n",
    "    # data['ADL'] = TA.ADL(data)\n",
    "\n",
    "    TSI = TA.TSI(ohlcv)\n",
    "    data['TSI'] = TSI[\"TSI\"]\n",
    "    data['TSIsignal'] = TSI[\"signal\"]\n",
    "\n",
    "    KST = TA.KST(ohlcv)\n",
    "    data['KST'] = KST[\"KST\"]\n",
    "\n",
    "    data['CHAIKIN'] = TA.CHAIKIN(ohlcv)\n",
    "    data['OBV'] = TA.OBV(ohlcv)\n",
    "    data['WOBV'] = TA.WOBV(ohlcv)\n",
    "\n",
    "    EBBP = TA.EBBP(ohlcv)\n",
    "    data['EBBPBull'] = EBBP[\"Bull.\"]\n",
    "    data['EBBPBear'] = EBBP[\"Bear.\"]\n",
    "\n",
    "    BASPN = TA.BASPN(ohlcv)\n",
    "    data['BASPNBuy'] = BASPN[\"Buy.\"]\n",
    "    data['BASPNSell'] = BASPN[\"Sell.\"]\n",
    "    data['COPP'] = TA.COPP(ohlcv)\n",
    "\n",
    "    BASP = TA.BASP(ohlcv)\n",
    "    data['BASPBuy'] = BASP[\"Buy.\"]\n",
    "    data['BASPSell'] = BASP[\"Sell.\"]\n",
    "\n",
    "    WTO = TA.WTO(ohlcv)\n",
    "    data['WTOWT1'] = WTO[\"WT1.\"]\n",
    "\n",
    "    data['STC'] = TA.STC(ohlcv)\n",
    "    data['VPT'] = TA.VPT(ohlcv)\n",
    "\n",
    "    # Có 50 kỹ thuật technical indicators ở đây\n",
    "    return data\n",
    "\n",
    "LOOK_BACK = 10\n",
    "\"\"\"Hàm này dùng để phân chia data ra theo từng ngày nối đuôi nhau vựa trên lock_back\"\"\"\n",
    "# Xem xem trung bình 10 ngày tăng hay giảm, là dùng để xem xu hướng của tiền\n",
    "\n",
    "def create_dataset(data, label, look_back=1):  #\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    # Tạo vòng lạp để lấy các lookback\n",
    "    for i in range(len(data)-look_back-1):\n",
    "        X_.append(data[i:(i+look_back)])\n",
    "        y_.append(label[i + look_back])\n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "\"\"\" Hàm này để lấy các ngày liên tiếp theo n_days để làm các cột dữ liệu\"\"\"\n",
    "\n",
    "def add_past_days_as_feature(data: pd.DataFrame, n_days: int = 5):\n",
    "    data = pd.concat([data.shift(i).add_suffix(f\"_{i}\") for i in range(n_days)], axis=1)\n",
    "    return data\n",
    "\n",
    "\" Hàm Bias dùng để so sánh giữ hai hai tính hiệu dài hạn và ngắn hạn vựa trên short_val và long_val \"\n",
    "def bias(prices):\n",
    "    short_avg = prices['Close'].rolling(3, min_periods=1).mean()\n",
    "    long_avg = prices['Close'].rolling(5, min_periods=1).mean()\n",
    "\n",
    "    short_val = pd.Series(((prices['Close'] - short_avg) / short_avg) * 100, name=\"BIAS_short\", index=prices.index)\n",
    "    long_val = pd.Series(((prices['Close'] - long_avg) / long_avg) * 100, name=\"BIAS_long\", index=prices.index)\n",
    "    indi = pd.concat([short_val, long_val], axis=1)\n",
    "\n",
    "    # So sánh xem dài hạn hay ngắn hạn cái nào sẽ lời hơn\n",
    "    signal = pd.Series((long_val > short_val).astype(int), name=\"BIAS_signal\", index=prices.index)\n",
    "    return indi, signal\n",
    "\n",
    "def vr(prices):\n",
    "    maximum = (prices['High'] + prices['Close'].shift(1).bfill()).mean() # Lấy giá cao nhất ngày hiện tạo cộng cho ngày đóng cửa của tương lai,\n",
    "    # Nếu tương lai là Nan thì cộng cho giá đóng cưa hiện tại\n",
    "    minimum = (prices['Low'] + prices['Close'].shift(1).bfill()).mean()\n",
    "    high = prices['High'].rolling(14, min_periods=1).mean()\n",
    "    low = prices['Low'].rolling(14, min_periods=1).mean()\n",
    "\n",
    "    # Tính chỉ số\n",
    "    indi = pd.Series((maximum - minimum) / (high - low), name=\"VR\", index=prices.index)\n",
    "    signal = pd.Series((indi > 0.5).astype(int), name=\"VR_signal\", index=prices.index)\n",
    "    return indi, signal\n",
    "\n",
    "# Hàm tính giá trị Trix\n",
    "def trix(prices):\n",
    "    indi = TA.TRIX(prices, 10)\n",
    "    signal = pd.Series((indi < 0).astype(int), name=\"TRIX_signal\", index=prices.index)\n",
    "    return indi, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "iJ_v7LojjUKC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# hàm này để đánh giá mô hình tổng\n",
    "# Là khi chúng ta train ra hết 3 model chúng ta xem rằng chúng kết hợp với nhau thì kết quả cuối cùng độ chính xác là bao nhiêu\n",
    "def CV_ensemble(ensemble_name, ensemble_func, estimators, X_train, y_train, n_folds=5, shuffle=True, random_state=2022):\n",
    "  kf = KFold(n_splits=5, random_state=random_state, shuffle=True) # Tạo số lượng dữ liệu bằng Kfold sau đó shuffle\n",
    "\n",
    "  res_list = [] # tạo res_list lưu thông tin của từng lần chia kfold\n",
    "  # Chạy tập dữ liệu Kfold\n",
    "  for train_idx, valid_idx in notebook.tqdm(kf.split(X_train), total=kf.get_n_splits(), desc='Eval_CV'): # Chạy theo số lần n_splits\n",
    "    X_train_train, X_valid = X_train[train_idx], X_train[valid_idx]\n",
    "    y_train_train, y_valid = y_train[train_idx], y_train[valid_idx]\n",
    "    # Tổng hợp kết quả test đối vơi từng cặp k riêng\n",
    "    ensemble_pred_proba = ensemble_func(estimators, X_train_train, y_train_train, X_valid)\n",
    "\n",
    "\n",
    "    # Lấy các đánh giá  của model với tập train test k này\n",
    "    neg_log_loss = np.negative(log_loss(y_valid, ensemble_pred_proba))\n",
    "    accuracy = accuracy_score(y_valid, ensemble_pred_proba.argmax(axis=1))\n",
    "\n",
    "    res_list.append([ensemble_name, neg_log_loss, accuracy]) # Lưu kết quả lại`\n",
    "  res_df = pd.DataFrame(np.vstack((res_list)))\n",
    "  res_df.columns = ['model', 'log_loss', 'accuracy']  # thêm các thông số để đánh giá\n",
    "  return res_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# hàm này để dự đoán bằng cách tổng hợp 3 model có train lại\n",
    "def ensemble_average(estimators, X_train, y_train, X_test):\n",
    "  # Hàm này vựa trên việc lấy xác suất của các model vote cái nào có xác suất cao nhất thì chọn\n",
    "  preds = []\n",
    "  num_estimators = len(estimators)\n",
    "  num_class = len(np.unique(y_train))\n",
    "  for iter in range(num_estimators):\n",
    "    y_train = np.array(y_train, dtype=np.int64)\n",
    "    estimators[iter].fit(X_train, y_train)\n",
    "    preds.append(estimators[iter].predict_proba(X_test))\n",
    "\n",
    "\n",
    "  preds_stack = np.hstack((preds))\n",
    "  preds_mean = []\n",
    "  for iter in range(num_class):\n",
    "    col_idx = np.arange(iter, num_estimators * num_class, num_class)\n",
    "    preds_mean.append(np.mean(preds_stack[:,col_idx], axis=1))\n",
    "\n",
    "  avg_pred = np.vstack((preds_mean)).transpose()\n",
    "  return avg_pred\n",
    "\n",
    "\n",
    "# hàm này đưa ra dự đoán mà ko cần train lại\n",
    "def ensemble_average_model(estimators, X_test, y_train): # esitamtors là các model và X_test là dữ liệu train\n",
    "  preds = []\n",
    "  num_estimators = len(estimators)\n",
    "  num_class = len(np.unique(y_train))\n",
    "  # đoạn này lấy tổng dữ đoán của các model trong estimators\n",
    "  for iter in range(num_estimators):\n",
    "\n",
    "    preds.append(estimators[iter].predict_proba(X_test))\n",
    "\n",
    "  preds_stack = np.hstack((preds))\n",
    "  preds_mean = []\n",
    "  # Dự đoán trung bình cho mỗi nhãn\n",
    "  for iter in range(num_class):\n",
    "    col_idx = np.arange(iter, num_estimators * num_class, num_class) # Tính cho số cột cho nhãn được dự đoán\n",
    "    preds_mean.append(np.mean(preds_stack[:,col_idx], axis=1)) # Tính giá trị trung bình cho dự đoán\n",
    "\n",
    "  avg_pred = np.vstack((preds_mean)).transpose() # Lấy hết các giá trị tring bình sau đó chuyển vị\n",
    "  return avg_pred\n",
    "# hàm tạo ngưỡng nhiễu\n",
    "def outlier_threshold(normality, k=1.5):\n",
    "  q1 = np.quantile(normality, 0.2)\n",
    "  q3 = np.quantile(normality, 0.8)\n",
    "  threshold = q1 - k*(q3-q1) # Công thưc tạo ngưỡng nhiễu\n",
    "  return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4qIHyxY0Jfu"
   },
   "source": [
    "# Khởi tạo và xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfAVHwSl0NBG",
    "outputId": "d3e24555-6aa9-42de-dc2a-675bec8464a6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_data(symbols, data_forex):\n",
    "  price = {}\n",
    "  # Lấy data của giá đóng cửa của loại đồng tiền\n",
    "  for symbol in data_forex.keys():\n",
    "      price[symbol] = data_forex[symbol]['Close'].copy()\n",
    "  y = {}\n",
    "  for symbol in data_forex.keys():\n",
    "      y[symbol] = data_forex[symbol][\"Close\"].ffill()\n",
    "  # x là features\n",
    "  X = {}\n",
    "  # Đoạn này lấy dữ liệu X là data dùng để tách train, test\n",
    "  for symbol in data_forex.keys():\n",
    "      X_base_features = create_indicators(data_forex[symbol])\n",
    "      X[symbol] = add_past_days_as_feature(data=X_base_features, n_days=LOOK_BACK)\n",
    "      X[symbol][\"label\"] = y[symbol].copy()\n",
    "      X[symbol] = X[symbol].dropna(axis=0)\n",
    "      y[symbol] = X[symbol][\"label\"].copy()\n",
    "  return X, y\n",
    "X, y  = create_data(symbols, data_forex)\n",
    "    # print(symbol, X[symbol].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "fQndo_Oh0vHO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "config = {\n",
    "    'data_name': '3d_forex',\n",
    "    'random_state': 2023\n",
    "}\n",
    "# Hàm này tách dữ liệu và sau đó chuẩn hóa riêng cho từng mã\n",
    "def split_data_scale_2(X: dict, y: dict, symbols: dict):\n",
    "    data_all_money = {}\n",
    "    for symbol in symbols[\"all\"]:\n",
    "        # Lấy dữ liệu gốc khi (chưa scale)\n",
    "        data_original = X[symbol].copy()\n",
    "        y_ = pd.Series(y[symbol])\n",
    "        # Khởi tạo biến để lưu dữ liệu train và test cho từng fold của từng loại tề\n",
    "        data_train_test = []\n",
    "\n",
    "        # Chia dữ liệu thành tập huấn luyện và tập kiểm tra theo tỷ lệ 90/10\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data_original, y_, test_size=0.1,shuffle =False, random_state=config['random_state'])\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "        data_all_money[symbol] = [X_train_scaled, X_test_scaled, y_train, y_test]\n",
    "    return data_all_money # Dữ liệu được lưu ở dạng dict và 1 loại đồng tiền chưa 4 dữ liệu train và test của X và y\n",
    "\n",
    "# data_all_money = split_data_scale_2(X, y, symbols) # Lấy dữ liệu đã scale ra\n",
    "\n",
    "\n",
    "\n",
    "#  Ta đi smooth data để smooth ta cần biết các dữ liệu nhiễu thường xuyên, và bộ dữ liệu thực tế\n",
    "# Việc cần ở đây là chỉ số nhiễu tượng trưng cho dữ liệu đó chỉ cần chỉnh bao nhiêu phần trăm bị nhiễu\n",
    "def Smoothing_data(y, symbols) -> dict:\n",
    "  symbolsmooths = {}\n",
    "  # cho nhiều bộ smooths y khác nhau để xem cái nào tốt nhất\n",
    "  for syms in symbols[\"all\"]:\n",
    "    smooths = []\n",
    "    for isf in range(4,5):\n",
    "        for ins in range(4,5):\n",
    "            smoothing_factor = isf\n",
    "            n_seasons = ins\n",
    "\n",
    "\n",
    "            # --- define state transition matrix A tạo ma trận n_season+1\n",
    "            state_transition = np.zeros((n_seasons+1, n_seasons+1))\n",
    "            # hidden level         Đặt mặc định số đầu tiên là 1\n",
    "            state_transition[0,0] = 1\n",
    "            # season cycle         Sau đó ta chỉnh các thông sô khác cho mặc định dòng 1 có n_season = -1\n",
    "            state_transition[1,1:-1] = [-1.0] * (n_seasons-1)\n",
    "\n",
    "            # Tạo đường chéo chính có giá trị là 1\n",
    "            state_transition[2:,1:-1] = np.eye(n_seasons-1)\n",
    "\n",
    "            # --- observation model H\n",
    "            # observation is hidden level + weekly seasonal compoenent\n",
    "            observation_model = [[1,1] + [0]*(n_seasons-1)]\n",
    "\n",
    "            # --- noise models, parametrized by the smoothing factor\n",
    "            level_noise = 0.2 / smoothing_factor\n",
    "            observation_noise = 0.2\n",
    "            season_noise = 1e-3\n",
    "\n",
    "            process_noise_cov = np.diag([level_noise, season_noise] + [0]*(n_seasons-1))**2\n",
    "            observation_noise_cov = observation_noise**2\n",
    "            # Sử dụng Kalman filter\n",
    "            kf = simdkalman.KalmanFilter(\n",
    "                state_transition, # Hiển thị cách model chuyển tiếp để thay đổi ( Đây là cách nhìn dữ liệu bị nhiễu)\n",
    "                process_noise_cov, # ma trận biểu thị mức độ nhiễu của mô hình (Đây là để biết nhiễu thường xảy ra với tuần suât nào)\n",
    "                observation_model, #  Là mô hình quan sát của hệ thông thường là ma trận hay hàm số biểu thị (Đây là  kết quả hiện tại đang cần chỉnh nhiễu)\n",
    "                observation_noise_cov #  Ma trận biểu thị mức độ nhiễu trong đo lường  (Đây giống như là sai số để xem kết quả dự đoán với thực tế thì sau bnhieu)\n",
    "                )\n",
    "            # Sau đó ta tính toán lấy ra được các dự đoán nhiễu và trã về dự đoán bị nhiễu được quy định bằng các tham số trên\n",
    "            block = y[syms]\n",
    "            n_train = block.shape[0]\n",
    "            n_test = 60\n",
    "            result = kf.compute(block, n_test)\n",
    "            predictproba = result.smoothed.states.mean[:,0]\n",
    "            y_label = []\n",
    "            for ivalue in range(1,len(predictproba)):\n",
    "                if(predictproba[ivalue] > predictproba[ivalue-1]):\n",
    "                    y_label.append(1)\n",
    "                else:\n",
    "                    y_label.append(-1)\n",
    "\n",
    "            smooths.append(y_label)\n",
    "    symbolsmooths[syms] = smooths\n",
    "  return symbolsmooths\n",
    "\n",
    "# symbolsmooths_y = Smoothing_data(y,symbols)\n",
    "\n",
    "\n",
    "def smooth_label(y_train_ ,symbolsmooths, syms ):\n",
    "  # Hàm chỉnh dữ liệu lại cho smooth\n",
    "  y_train = []\n",
    "  y_test = []\n",
    "  # chạy từng mã gán lại nhãn đã smooth, -1 là 0 giảm, 1 là 1 tăng\n",
    "  for idx in range(len(y_train_)):\n",
    "      if symbolsmooths[syms][0][idx]  == -1:\n",
    "          y_train.append(0)\n",
    "      else:\n",
    "          y_train.append(1)\n",
    "  # y_test cũng vậy, phần test sẽ là phần còn lại tính từ n_split\n",
    "  for idx in range(len(symbolsmooths[syms][0]) - len(y_train_)):\n",
    "      if symbolsmooths[syms][0][idx+ len(y_train_)] == -1:\n",
    "          y_test.append(0)\n",
    "      else:\n",
    "          y_test.append(1)\n",
    "  y_test.append(1)\n",
    "  return y_train, y_test\n",
    "\n",
    "\n",
    "def create_train_test_all_data_forex(X, y, symbols):\n",
    "  data_all = {}\n",
    "  data_all_money = split_data_scale_2(X,y, symbols)\n",
    "  symbolsmooth_y = Smoothing_data(y, symbols)\n",
    "\n",
    "  for sym in symbols[\"all\"]:\n",
    "    # Lấy các cột của dữ liệu\n",
    "    df = pd.DataFrame(X[sym] )\n",
    "    col_df = df.columns\n",
    "\n",
    "    # Lấy đúng loại tiền cần dùng\n",
    "    X_train, X_test, y_train, y_test = data_all_money[sym]\n",
    "    # Sau khi smooth data ta có y_train và y_test mới\n",
    "    y_train, y_test = smooth_label( y_train, symbolsmooth_y, sym)\n",
    "\n",
    "    # Sau đó  tạo ra các dataframe mới\n",
    "    X_train = pd.DataFrame(X_train, columns = col_df)\n",
    "    X_test =  pd.DataFrame(X_test, columns = col_df)\n",
    "    # Drop các cột không cần thiết là label đi\n",
    "    X_train = X_train.drop(columns = [\"label\"])\n",
    "    X_test = X_test.drop(columns = [\"label\"])\n",
    "    # Lấy dữ liệu tiền tệ đó ra\n",
    "    data_set = pd.get_dummies(X_train, drop_first=False)\n",
    "    data_all[sym] = [data_set, X_test, y_train, y_test]\n",
    "  return data_all\n",
    "\n",
    "data_all = create_train_test_all_data_forex(X, y, symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ROVcHBG2JxmL"
   },
   "outputs": [],
   "source": [
    "# data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw_g-vMT3myc"
   },
   "source": [
    "# Tuning hyperparameter cho các model và loại bỏ outlier trong dữ liệu:\n",
    "1. Random forest\n",
    "2. XGBoost\n",
    "3. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Vm_dBd3xP8Wl"
   },
   "outputs": [],
   "source": [
    "def create_clf(X_train):\n",
    "  clf = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    n_jobs=-1,\n",
    "    random_state=config['random_state'])\n",
    "\n",
    "  clf.fit(X_train)\n",
    "  return clf\n",
    "\n",
    "\n",
    "# Hàm loại bỏ outlier cho tập data train\n",
    "def delete_outlier(clf, X_train, y_train):\n",
    "  # Lấy ngưỡng cho dữ liệu\n",
    "  normality_df = pd.DataFrame(clf.decision_function(X_train), columns=['normality'])\n",
    "  threshold = outlier_threshold(normality_df['normality'].values, k=1.5)\n",
    "\n",
    "  # Sau đo ta loại bỏ dòng bị outlier\n",
    "  X_train = X_train[normality_df['normality'].values>=threshold]\n",
    "  y_train = y_train[normality_df['normality'].values>=threshold]\n",
    "  return X_train, y_train\n",
    "\n",
    "# hàm này tạo ra dataframe chưa bộ thông số tốt nhất cho từng model\n",
    "def create_best_cv(model_name):\n",
    "\n",
    "  model_list = []\n",
    "  # mỗi model chuẩn bị 5 cái loss tốt nhất vì chạy CV\n",
    "  for name in model_name:\n",
    "    model_list.append(np.full(5, name))\n",
    "\n",
    "  best_cv_df = pd.DataFrame({'model': np.hstack((model_list)), 'accuracy':None, 'best_hyper_param':None})\n",
    "  return best_cv_df\n",
    "\n",
    "def data_np(data_all, symbols):\n",
    "  for sym in symbols[\"all\"]:\n",
    "    # Thay đổi dữ liệu về dạng làm tròn và numpy\n",
    "\n",
    "    data_all[sym][2] = np.array(data_all[sym][2])\n",
    "    data_all[sym][3] = np.array(data_all[sym][3])\n",
    "\n",
    "    data_all[sym][0] = data_all[sym][0].astype(np.float32)\n",
    "    data_all[sym][1] = data_all[sym][1].astype(np.float32)\n",
    "\n",
    "def delete_and_create_best(data_all, symbols):\n",
    "  data_np(data_all, symbols)\n",
    "  best_cv_df = {}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    # Tạo clf cho delete outlier\n",
    "    clf = create_clf(data_all[sym][0])\n",
    "    # xóa các outliers\n",
    "    print(sym)\n",
    "    data_all[sym][0], data_all[sym][2] = delete_outlier(clf, data_all[sym][0], data_all[sym][2] )\n",
    "    # Tạo ra best cv cho từng mã tiền\n",
    "    model_name = [ 'rf', 'xgb', 'mlp']\n",
    "    best_cv_df[sym] = create_best_cv(model_name)\n",
    "  return best_cv_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vZe2UvjRSsl",
    "outputId": "9230be9d-0cd9-4340-d78a-b9545bdd2dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDCHF\n"
     ]
    }
   ],
   "source": [
    "best_cv_df = delete_and_create_best(data_all, symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "W-zbqbca31_R"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def optimizer_rf_model(X_train, y_train, best_cv_df, sym):\n",
    "  #from keras.callbacks import EarlyStopping\n",
    "  param_grid = {\n",
    "    'n_estimators': [200,300,400],\n",
    "    'max_depth' : [8,10],\n",
    "    'criterion' :['entropy']\n",
    "  }\n",
    "  rfc=RandomForestClassifier(random_state=23)\n",
    "  tune_search = GridSearchCV(estimator=rfc, param_grid=param_grid,n_jobs =6,scoring='accuracy', verbose =2,refit='accuracy',cv= 3)\n",
    "  tune_search.fit(X_train, y_train)\n",
    "  model_name = 'rf'\n",
    "  accuracies = tune_search.cv_results_[\"mean_test_score\"]\n",
    "  best_hyperparameter = tune_search.cv_results_[\"params\"]\n",
    "  # Lấy ra 5 parameter tốt nhất\n",
    "  top_5_indices = (-accuracies).argsort()[:5]\n",
    "  top_5_hyperparameter = [best_hyperparameter[i] for i in top_5_indices]\n",
    "  cv_df = pd.DataFrame(tune_search.cv_results_)\n",
    "  # Lấy trung bình dự đoán độ chinh xác khi trên tập kfold của tùng best hyperparameter\n",
    "  acc = []\n",
    "  for i in range(len(cv_df)):\n",
    "    if cv_df[\"params\"].values[i] in top_5_hyperparameter:\n",
    "      a = [cv_df[\"split0_test_score\"].values[i], cv_df[\"split1_test_score\"].values[i], cv_df[\"split2_test_score\"].values[i]]\n",
    "      acc.append(round(sum(a)/3, 5)) # Tính trung bình cho từng hyperparameter khi làm xong việc test fold = 3\n",
    "  best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'best_hyper_param'] = top_5_hyperparameter\n",
    "  best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'accuracy'] = acc\n",
    "  # lưu lại 5 model tốt nhất bằng random forest\n",
    "  return tune_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB được tối ưu bằng Grid search\n",
    "def optimizer_xgb_model(X_train, y_train, best_cv_df, sym):\n",
    "  parameters = {\n",
    "      # Các Hyper parameter cần tính toán\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': [0.1,0.05],\n",
    "    'min_child_weight': [5],\n",
    "    'gamma': [0.5],\n",
    "    'subsample': [0.6],\n",
    "    'colsample_bytree': [1.0, 2],\n",
    "    'max_depth': [5,8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'use_label_encoder': [False],\n",
    "    'random_state': [config['random_state']]\n",
    "}\n",
    "  xgb_model = XGBClassifier()\n",
    "\n",
    "  grid_search = GridSearchCV(\n",
    "      estimator=xgb_model,\n",
    "      param_grid=parameters,\n",
    "      scoring = 'accuracy',\n",
    "      n_jobs = 10,\n",
    "      cv = 3,\n",
    "      verbose=2\n",
    "  )\n",
    "\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  model_name = 'xgb'\n",
    "  accuracies = grid_search.cv_results_[\"mean_test_score\"]\n",
    "  best_hyperparameter = grid_search.cv_results_[\"params\"]\n",
    "  # Lấy ra 5 parameter tốt nhất\n",
    "  top_5_indices = (-accuracies).argsort()[:5]\n",
    "  top_5_hyperparameter = [best_hyperparameter[i] for i in top_5_indices]\n",
    "  cv_df = pd.DataFrame(grid_search.cv_results_)\n",
    "  # Lấy trung bình dự đoán độ chinh xác khi trên tập kfold của tùng best hyperparameter\n",
    "  acc = []\n",
    "  for i in range(len(cv_df)):\n",
    "      if cv_df[\"params\"].values[i] in top_5_hyperparameter:\n",
    "        a = [cv_df[\"split0_test_score\"].values[i], cv_df[\"split1_test_score\"].values[i], cv_df[\"split2_test_score\"].values[i]]\n",
    "        acc.append(round(sum(a)/3, 5)) # Tính trung bình cho từng hyperparameter khi làm xong việc test fold = 3\n",
    "  best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'best_hyper_param'] = top_5_hyperparameter\n",
    "  best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'accuracy'] = acc\n",
    "  return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_cv_rf_and_xgb(data_all, symbols, best_cv_df):\n",
    "  tune_and_grid = {}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    X_train = data_all[sym][0]\n",
    "    y_train = data_all[sym][2]\n",
    "    # lấy các search của tất cả model\n",
    "    tune_search = optimizer_rf_model(X_train, y_train, best_cv_df, sym)\n",
    "    grid_search = optimizer_xgb_model(X_train, y_train, best_cv_df, sym)\n",
    "    tune_and_grid[sym] = [tune_search, grid_search]\n",
    "  return tune_and_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZd_whSYwPnD",
    "outputId": "e502c39d-451d-4733-f111-a4cf2eee1eb6"
   },
   "outputs": [],
   "source": [
    "# Hàm tối ưu MLP bằng Grid_search\n",
    "# Hàm dưới đây dung để Tuning hyperparameter cho model MLP\n",
    "def optimize_model_MLP(X_train, y_train, best_cv_df, sym):\n",
    "  mlp_clf = MLPClassifier()\n",
    "  param_grid = {\n",
    "      'hidden_layer_sizes': [(120,80,40),(100,60,30)],\n",
    "      'max_iter': [80, 100],\n",
    "      'activation': ['tanh'],\n",
    "      'solver': ['sgd'],\n",
    "      'alpha': [0.05,0.1],  # Chỉ số cần được chỉnh\n",
    "      'learning_rate': ['constant','adaptive'],\n",
    "  }\n",
    "  # Đoạn dưới đây là chạy Grid search cho model\n",
    "  grid_search = GridSearchCV(mlp_clf, param_grid, n_jobs= 6, cv=3)\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  model_name = 'mlp'\n",
    "  accuracies = grid_search.cv_results_[\"mean_test_score\"]\n",
    "  best_hyperparameter = grid_search.cv_results_[\"params\"]\n",
    "  # Lấy ra 5 parameter tốt nhất\n",
    "  top_5_indices = (-accuracies).argsort()[:5]\n",
    "  top_5_hyperparameter = [best_hyperparameter[i] for i in top_5_indices]\n",
    "  cv_df = pd.DataFrame(grid_search.cv_results_)\n",
    "  # Lấy trung bình dự đoán độ chinh xác khi trên tập kfold của tùng best hyperparameter\n",
    "  acc = []\n",
    "  for i in range(len(cv_df)):\n",
    "    if cv_df[\"params\"].values[i] in top_5_hyperparameter:\n",
    "      a = [cv_df[\"split0_test_score\"].values[i], cv_df[\"split1_test_score\"].values[i], cv_df[\"split2_test_score\"].values[i]]\n",
    "      acc.append(round(sum(a)/3, 5)) # Tính trung bình cho từng hyperparameter khi làm xong việc test fold = 3\n",
    "  best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'best_hyper_param'] = top_5_hyperparameter\n",
    "  best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'accuracy'] = acc\n",
    "  return grid_search\n",
    "\n",
    "# Hàm này lấy ra tất cả grid tốt nhaas và model mlp_clf đã thực hiện\n",
    "def result_optimize_MLP(data_all,best_cv_df, symbols):\n",
    "  grid_all = {}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    X_train = data_all[sym][0]\n",
    "    y_train = data_all[sym][2]\n",
    "    grid= optimize_model_MLP(X_train, y_train, best_cv_df, sym)\n",
    "    grid_all[sym] = grid\n",
    "  return grid_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_hPwF6W3Bs6"
   },
   "source": [
    "# Note cần lưu lại tập tốt nhất và lưu lại model tốt nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CaQHb5MI2YOB"
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# for sym in symbols[\"all\"]:\n",
    "#   grid = grid_all[sym]\n",
    "#   # model_name = 'mlp'\n",
    "\n",
    "#   # best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'best_hyper_param'] = str(grid.best_params_)\n",
    "\n",
    "#   cv_df = pd.DataFrame(grid.cv_results_)\n",
    "#   # cv_values = cv_df.loc[grid.best_index_, cv_df.columns.str.startswith('split')].values\n",
    "#   # best_cv_df[sym].loc[best_cv_df[sym]['model']==model_name, 'accuracy'] = cv_values[:5]\n",
    "\n",
    "#   # thu được 5 model tốt nhất MLP\n",
    "#   tune_result_df = pd.concat([pd.DataFrame(grid.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\n",
    "#   print(sym, \"--------------------\")\n",
    "\n",
    "#   fig = px.parallel_coordinates(tune_result_df, color='mean_test_score')\n",
    "#   fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kn8Jv6k1oAZ"
   },
   "source": [
    "# Kết hợp tất cả các model sau khi chạy kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "csldt11v1uUz"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "def best_model_stack(symbols, data_all, best_cv_df):\n",
    "  estimators_all ={}\n",
    "  res_df_all = {}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    X_train = data_all[sym][0]\n",
    "    y_train = data_all[sym][2]\n",
    "    X = np.array(X_train)\n",
    "    y = np.array(y_train, dtype=np.int64)\n",
    "    # DƯới đây là code vét cạn model\n",
    "    for i_rf in range(0,1):\n",
    "      for i_xgb in range(0,1):\n",
    "        for i_mlp in range(0,1):\n",
    "          # load 3 cấu hình tốt nhất cho 3 model vào biến rf xbg và mlp\n",
    "          rf1 = RandomForestClassifier(**best_cv_df[sym].loc[best_cv_df[sym]['model']=='rf', 'best_hyper_param'].values[i_rf])\n",
    "          xgb1 = XGBClassifier(**best_cv_df[sym].loc[best_cv_df[sym]['model']=='xgb', 'best_hyper_param'].values[i_xgb])\n",
    "          mlp1 = MLPClassifier(**best_cv_df[sym].loc[best_cv_df[sym]['model']=='mlp', 'best_hyper_param'].values[i_mlp])\n",
    "          # quá trình tổng hopkw 3 model để tạo ra model trung bình tốt nhất\n",
    "          estimators = [rf1, xgb1, mlp1]\n",
    "          estimators_name = 'rf_xgb_mlp'\n",
    "          ensemble_name = 'average' + '_by_' + estimators_name + str(i_rf) + str(i_xgb) + str(i_mlp)\n",
    "          res_df = CV_ensemble(ensemble_name, ensemble_average, estimators, X, y, n_folds=5, shuffle=True, random_state=config['random_state'])\n",
    "          sym_brute_force = sym + str(i_rf) + str(i_xgb) + str(i_mlp)\n",
    "          res_df_all[sym_brute_force] = res_df\n",
    "          estimators_all[sym_brute_force] = estimators\n",
    "          print(sym_brute_force)\n",
    "          print(res_df)\n",
    "    return res_df_all, estimators_all# Trả về những model vét cạn lại với nhau\n",
    "# mô hình tốt nhất khi kết hợp 3 model khi chạy xong n_folds\n",
    "\n",
    "# res_df_all,  estimators_all =  best_model_stack(symbols, data_all, best_cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "XSyLlEsYYOBq"
   },
   "outputs": [],
   "source": [
    "def best_model_ens(res_df_all, symbols):\n",
    "    keys = res_df_all.keys()\n",
    "    best_model_all = {}  # Lấy model tốt nhất từ các model stack lại với nhau.\n",
    "    for sym in symbols[\"all\"]:\n",
    "        model_max = 0\n",
    "        symb = None\n",
    "        for key in keys:  # Key chứa dữ liệu của các model stack lại với nhau và chưa luôn việc các cặp stack của model với nhau\n",
    "            if sym in key:  # So sánh đúng là model thì ta tiếp tục\n",
    "                candidate = res_df_all[key]['accuracy'].astype(float).mean()\n",
    "                print(candidate)\n",
    "                if candidate > model_max:\n",
    "                    model_max = candidate\n",
    "                    symb = key\n",
    "        if symb is not None:\n",
    "            best_model_all[sym] = {\n",
    "                'model': res_df_all[symb]['model'].values[0],\n",
    "                'accuracy': model_max  # Lưu giá trị accuracy\n",
    "            }\n",
    "    return best_model_all # Lấy ra cụ thể ở model nào mô hình nào tốt nhất vựa trên 3 con số cuối kết hợp với nhau\n",
    "# best_model_all = best_model_ens(res_df_all, symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "YYWz3bHhJVsQ"
   },
   "outputs": [],
   "source": [
    "def tuning_hyperparameter(data_all, symbols, best_cv_df):\n",
    "  result_tune_and_grid = best_cv_rf_and_xgb(data_all, symbols, best_cv_df)\n",
    "  grid_all = result_optimize_MLP(data_all, best_cv_df, symbols)\n",
    "def check_tuning(symbols,best_cv_df):\n",
    "  \"\"\"Symbos chưa tất cả các mã của tiền tệ\n",
    "  best_cv_df xem tập tuning của model để xem nó đã có hay chưa\n",
    "  return trả về 1 cái list chứa sự tồn tại của từng mã\n",
    "  \"\"\"\n",
    "  # Hàm này check xem symbols đó đã có hay chưa nếu chưa thì tuning cho model\n",
    "  check_tun = {}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    if best_cv_df[sym] is not None:\n",
    "      check_tun[sym] = True\n",
    "    else:\n",
    "      check_tun[sym] =  False\n",
    "  return check_tun\n",
    "def check_model(symbols, estimators_all):\n",
    "  # Hàm này check xem model đó có tồn tại hay không\n",
    "  check ={}\n",
    "  for sym in symbols[\"all\"]:\n",
    "    # Check xem model đó đã tồn tại hãy chưa\n",
    "    if estimators_all[sym] is not None:\n",
    "      check[sym] = True\n",
    "    else:\n",
    "      check[sym] =  False\n",
    "  return check\n",
    "def train_model(sym, best_cv, X_train, y_train,best_index = \"000\"):\n",
    "  \"\"\"hÀM NÀy train model cân muốn train và có chỉ số cụ thể tốt nhất \"\"\"\n",
    "  i_rf = int(best_index[0])\n",
    "  i_xgb = int(best_index[1])\n",
    "  i_mlp = int(best_index[2])\n",
    "  rf1 = RandomForestClassifier(**best_cv_df[sym].loc[best_cv_df[sym]['model']=='rf', 'best_hyper_param'].values[i_rf])\n",
    "  xgb1 = XGBClassifier(**best_cv_df[sym].loc[best_cv_df[sym]['model']=='xgb', 'best_hyper_param'].values[i_xgb])\n",
    "  mlp_clf = MLPClassifier()\n",
    "  mlp1 = mlp_clf.set_params(**best_cv_df[sym].loc[best_cv_df[sym]['model']=='mlp', 'best_hyper_param'].values[i_mlp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "svRFVYwC8-LS"
   },
   "outputs": [],
   "source": [
    "def predict_all(estimators_all, data_all, symbols):\n",
    "    pred_y_label_all = {}\n",
    "    for sym in symbols[\"all\"]:\n",
    "        if sym in data_all:\n",
    "            estimators = estimators_all[sym + \"000\"]\n",
    "            X_test = data_all[sym][1].copy()\n",
    "            y_train = data_all[sym][2].copy()\n",
    "            pred_y_label = ensemble_average_model(estimators, X_test, y_train)\n",
    "            pred_y_label = pred_y_label.argmax(axis=1)\n",
    "            pred_y_label_all[sym] = pred_y_label\n",
    "        else:\n",
    "            # Xử lý khi sym không tồn tại trong data_all\n",
    "            print(f\"KeyError: Symbol {sym} not found in data_all\")\n",
    "    return pred_y_label_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c45ff6e21f854b1cbd4582c61c399ca0",
      "7c3e702694d94497830591d08b605624",
      "825c0180479d490db631c4de12cf41b5",
      "a3c613552c1846669c170000fa9e6066",
      "ed21f28b030a4b888e7f675b6da07a7c",
      "8f9a07505bf74695babd81403fa3931c",
      "a7048fc165624ffeb49363f96f653e35",
      "74ab268cec6d477cabf8d35aba762a21",
      "2cd11cf1f27c4de7892fa2ac1fb78ab6",
      "36d20d303d824d31aa792b03d9695a73",
      "77a02de06a014627891566ba8f20566a",
      "c9a6eb0831aa49ddbdf60f9b784d682a",
      "2396d664c71d4ebc96aebab2288f6ccd",
      "5d3c34a36fa94b0b94669dd5358167c1",
      "6923ffb7254d4f45ad9dedbd12ee2247",
      "f840a2ecf5b8498e87a9087267519c6e",
      "03af047b63c442f18ebce6e50b52dd61",
      "bf018eaab2a74744926767c1fe9b15e4",
      "16c7eb89f33f44bc8af117445df87ba5",
      "6c0dada3abb34afa8af80d27f644a944",
      "81fdedd1fb144f47adc5adf9c2516133",
      "2efe9c0449554c0ea885b4bc758afaed"
     ]
    },
    "id": "nbSHmB09G2Qd",
    "outputId": "b01d6111-ac62-4bec-cf64-52bf0a3c3b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDCHF\n",
      "USDCHF\n",
      "{'USDCHF':    model accuracy best_hyper_param\n",
      "0     rf     None             None\n",
      "1     rf     None             None\n",
      "2     rf     None             None\n",
      "3     rf     None             None\n",
      "4     rf     None             None\n",
      "5    xgb     None             None\n",
      "6    xgb     None             None\n",
      "7    xgb     None             None\n",
      "8    xgb     None             None\n",
      "9    xgb     None             None\n",
      "10   mlp     None             None\n",
      "11   mlp     None             None\n",
      "12   mlp     None             None\n",
      "13   mlp     None             None\n",
      "14   mlp     None             None}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd12faf8612844cbb6415dfc832c0c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval_CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDCHF000\n",
      "                      model              log_loss            accuracy\n",
      "0  average_by_rf_xgb_mlp000   -0.4779648190944229  0.7894736842105263\n",
      "1  average_by_rf_xgb_mlp000  -0.44412003948451423  0.8170426065162907\n",
      "2  average_by_rf_xgb_mlp000   -0.4583936731049038  0.8170426065162907\n",
      "3  average_by_rf_xgb_mlp000   -0.4241795817116988  0.8467336683417085\n",
      "4  average_by_rf_xgb_mlp000   -0.4472632148729563   0.821608040201005\n",
      "0.8183801211571643\n"
     ]
    }
   ],
   "source": [
    "# symbols = {\"all\": [\"EURCHF\"]}\n",
    "# def exists_best_cv(file_path = \"best_cv_df.pkl\", data_all = data_all, symbols = symbols):\n",
    "#   best_cv_df = delete_and_create_best(data_all, symbols)\n",
    "#   if os.path.exists(file_path):\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#       best_cv_df = pickle.load(file)\n",
    "#     return best_cv_df, True\n",
    "#   else:\n",
    "#     return best_cv_df, False\n",
    "# def check_exists_model():\n",
    "#   pass\n",
    "def predict_signal(symbols):\n",
    "  data_forex = get_data(symbols)\n",
    "  X,y = create_data(symbols, data_forex.copy())\n",
    "  data_all = create_train_test_all_data_forex(X, y, symbols)\n",
    "  # best_cv_df, check = exists_best_cv(file_path = \"best_cv_df.pkl\", data_all = data_all, symbols = symbols)\n",
    "  best_cv_df = delete_and_create_best(data_all, symbols)\n",
    "  print(best_cv_df)\n",
    "  tuning_hyperparameter(data_all, symbols, best_cv_df)\n",
    "  with open(\"best_cv_df.pkl\", 'wb') as file:\n",
    "    pickle.dump(best_cv_df, file)\n",
    "  res_df_all, estimators_all=  best_model_stack(symbols, data_all, best_cv_df)\n",
    "  best_model_all = best_model_ens(res_df_all, symbols)\n",
    "  with open(\"best_model_all.pkl\", 'wb') as file:\n",
    "        pickle.dump(best_model_all, file)\n",
    "  predict = predict_all(estimators_all, data_all, symbols)\n",
    "  return predict, best_model_all\n",
    "predict, best_model = predict_signal(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MusWcx3xZEDC",
    "outputId": "4d49370f-bfe6-4bc9-f3d2-73281a3d0c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USDCHF': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USDCHF': {'model': 'average_by_rf_xgb_mlp000',\n",
       "  'accuracy': 0.8183801211571643}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 1.5.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: The Pandas Development Team\n",
      "Author-email: pandas-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\n",
      "Requires: numpy, numpy, python-dateutil, pytz\n",
      "Required-by: altair, bokeh, catboost, datashader, finta, holoviews, hvplot, medmnist, mizani, panel, plotnine, pmdarima, seaborn, statsmodels, streamlit, ta, xarray\n"
     ]
    }
   ],
   "source": [
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FPT':    model accuracy                                   best_hyper_param\n",
      "0     rf  0.67119  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "1     rf  0.67107  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "2     rf  0.67108  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "3     rf  0.67042  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "4     rf  0.67012  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "5    xgb  0.63638  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "6    xgb  0.63687  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "7    xgb  0.65213  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "8    xgb  0.64781  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "9    xgb      NaN  {'colsample_bytree': 2, 'gamma': 0.5, 'learnin...\n",
      "10   mlp  0.66871  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...\n",
      "11   mlp  0.66113  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "12   mlp  0.66289  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...\n",
      "13   mlp  0.66404  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "14   mlp  0.66146  {'activation': 'tanh', 'alpha': 0.05, 'hidden_..., 'PNJ':    model accuracy                                   best_hyper_param\n",
      "0     rf  0.66292  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "1     rf  0.66361  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "2     rf  0.66523  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "3     rf  0.66564  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "4     rf  0.66552  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "5    xgb  0.67456  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "6    xgb  0.66305  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "7    xgb  0.67664  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "8    xgb   0.6753  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "9    xgb      NaN  {'colsample_bytree': 2, 'gamma': 0.5, 'learnin...\n",
      "10   mlp  0.65649  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "11   mlp  0.65881  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "12   mlp  0.65765  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...\n",
      "13   mlp   0.6601  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "14   mlp  0.65894  {'activation': 'tanh', 'alpha': 0.05, 'hidden_..., 'MSN':    model accuracy                                   best_hyper_param\n",
      "0     rf  0.60982  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "1     rf   0.6099  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "2     rf  0.61035  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "3     rf  0.61042  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "4     rf  0.61034  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "5    xgb  0.60131  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "6    xgb  0.59423  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "7    xgb  0.61065  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "8    xgb  0.60708  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "9    xgb      NaN  {'colsample_bytree': 2, 'gamma': 0.5, 'learnin...\n",
      "10   mlp  0.61324  {'activation': 'tanh', 'alpha': 0.05, 'hidden_...\n",
      "11   mlp  0.60852  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "12   mlp  0.60933  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "13   mlp  0.60577  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "14   mlp  0.60521  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l..., 'VIC':    model accuracy                                   best_hyper_param\n",
      "0     rf  0.62685  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "1     rf  0.62695  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "2     rf  0.62658  {'criterion': 'entropy', 'max_depth': 8, 'n_es...\n",
      "3     rf  0.62619  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "4     rf   0.6264  {'criterion': 'entropy', 'max_depth': 10, 'n_e...\n",
      "5    xgb  0.62084  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "6    xgb  0.61182  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "7    xgb   0.6308  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "8    xgb  0.62926  {'colsample_bytree': 1.0, 'gamma': 0.5, 'learn...\n",
      "9    xgb      NaN  {'colsample_bytree': 2, 'gamma': 0.5, 'learnin...\n",
      "10   mlp  0.63065  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "11   mlp  0.63393  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "12   mlp  0.63389  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "13   mlp  0.63397  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...\n",
      "14   mlp  0.63126  {'activation': 'tanh', 'alpha': 0.1, 'hidden_l...}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the file in binary read mode\n",
    "with open('D:/ForexResearch/best_cv_df.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the deserialized object from the .pkl file\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03af047b63c442f18ebce6e50b52dd61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16c7eb89f33f44bc8af117445df87ba5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2396d664c71d4ebc96aebab2288f6ccd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03af047b63c442f18ebce6e50b52dd61",
      "placeholder": "​",
      "style": "IPY_MODEL_bf018eaab2a74744926767c1fe9b15e4",
      "value": "Eval_CV: 100%"
     }
    },
    "2cd11cf1f27c4de7892fa2ac1fb78ab6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2efe9c0449554c0ea885b4bc758afaed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36d20d303d824d31aa792b03d9695a73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d3c34a36fa94b0b94669dd5358167c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16c7eb89f33f44bc8af117445df87ba5",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c0dada3abb34afa8af80d27f644a944",
      "value": 5
     }
    },
    "6923ffb7254d4f45ad9dedbd12ee2247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81fdedd1fb144f47adc5adf9c2516133",
      "placeholder": "​",
      "style": "IPY_MODEL_2efe9c0449554c0ea885b4bc758afaed",
      "value": " 5/5 [01:34&lt;00:00, 18.75s/it]"
     }
    },
    "6c0dada3abb34afa8af80d27f644a944": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74ab268cec6d477cabf8d35aba762a21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77a02de06a014627891566ba8f20566a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c3e702694d94497830591d08b605624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f9a07505bf74695babd81403fa3931c",
      "placeholder": "​",
      "style": "IPY_MODEL_a7048fc165624ffeb49363f96f653e35",
      "value": "Eval_CV: 100%"
     }
    },
    "81fdedd1fb144f47adc5adf9c2516133": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "825c0180479d490db631c4de12cf41b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74ab268cec6d477cabf8d35aba762a21",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2cd11cf1f27c4de7892fa2ac1fb78ab6",
      "value": 5
     }
    },
    "8f9a07505bf74695babd81403fa3931c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3c613552c1846669c170000fa9e6066": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36d20d303d824d31aa792b03d9695a73",
      "placeholder": "​",
      "style": "IPY_MODEL_77a02de06a014627891566ba8f20566a",
      "value": " 5/5 [01:33&lt;00:00, 18.55s/it]"
     }
    },
    "a7048fc165624ffeb49363f96f653e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf018eaab2a74744926767c1fe9b15e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c45ff6e21f854b1cbd4582c61c399ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c3e702694d94497830591d08b605624",
       "IPY_MODEL_825c0180479d490db631c4de12cf41b5",
       "IPY_MODEL_a3c613552c1846669c170000fa9e6066"
      ],
      "layout": "IPY_MODEL_ed21f28b030a4b888e7f675b6da07a7c"
     }
    },
    "c9a6eb0831aa49ddbdf60f9b784d682a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2396d664c71d4ebc96aebab2288f6ccd",
       "IPY_MODEL_5d3c34a36fa94b0b94669dd5358167c1",
       "IPY_MODEL_6923ffb7254d4f45ad9dedbd12ee2247"
      ],
      "layout": "IPY_MODEL_f840a2ecf5b8498e87a9087267519c6e"
     }
    },
    "ed21f28b030a4b888e7f675b6da07a7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f840a2ecf5b8498e87a9087267519c6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
