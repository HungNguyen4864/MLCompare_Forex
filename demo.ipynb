{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from finta import TA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import simdkalman\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(best_model_tuning):\n",
    "    symbols = {}\n",
    "    list_sym = []\n",
    "    for sym in best_model_tuning.keys():\n",
    "        list_sym.append(sym)\n",
    "    symbols['all'] = list_sym\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keysymbol(best_model_tuning):\n",
    "    key_symbols = {}\n",
    "    for sym in best_model_tuning.keys():\n",
    "        best_cv = list(best_model_tuning[sym].keys())\n",
    "        key_symbols[sym] = best_cv\n",
    "    return key_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sym_model():\n",
    "    with open ('best_model_tuning.pkl','rb') as output:\n",
    "        best_model_tuning = pickle.load(output)\n",
    "    key_symbols = get_keysymbol(best_model_tuning)\n",
    "    return best_model_tuning,key_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(symbols: list, timeframe: str = \"D1\"):\n",
    "    start = \"2018-01-01\"\n",
    "    end = datetime.now().date().isoformat()\n",
    "    data_new = {}\n",
    "    params = {\n",
    "            \"timeframe\": timeframe,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "        }\n",
    "    for sym in symbols['all']:\n",
    "        print(sym)\n",
    "        response = requests.get(f\"http://221.132.33.180:8005/history/{sym}\", params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        data = pd.DataFrame(data)\n",
    "        data[\"Date\"] = pd.to_datetime(data['Date'], unit='s')\n",
    "        data.set_index(\"Date\", inplace=True)\n",
    "        data_new[sym] = data\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indicators(ohlcv: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = ohlcv.copy()\n",
    "    indi, signal = bias(ohlcv)\n",
    "    data = pd.concat([data, indi], axis=1)\n",
    "    data['BIAS_signal'] = signal\n",
    "    data['VR'], data[\"VR_signal\"] = vr(ohlcv)\n",
    "    data['TRIX'], data[\"TRIX_signal\"] = trix(ohlcv)\n",
    "    data['ER'] = TA.ER(ohlcv)\n",
    "    data['EVWMA'] = TA.EVWMA(ohlcv)\n",
    "    data['VWAP'] = TA.VWAP(ohlcv)\n",
    "    data['MOM'] = TA.MOM(ohlcv)\n",
    "    data['ROC'] = TA.ROC(ohlcv)\n",
    "    data['RSI'] = TA.RSI(ohlcv)\n",
    "    data['IFT_RSI'] = TA.IFT_RSI(ohlcv)\n",
    "    data['ATR'] = TA.ATR(ohlcv)\n",
    "    data['BBWIDTH'] = TA.BBWIDTH(ohlcv)\n",
    "    data['ADX'] = TA.ADX(ohlcv)\n",
    "    data['STOCH'] = TA.STOCH(ohlcv)\n",
    "    data['STOCHD'] = TA.STOCHD(ohlcv)\n",
    "    data['AO'] = TA.AO(ohlcv)\n",
    "    data['MI'] = TA.MI(ohlcv)\n",
    "    data['MFI'] = TA.MFI(ohlcv)\n",
    "    data['PZO'] = TA.PZO(ohlcv)\n",
    "    data['EFI'] = TA.EFI(ohlcv)\n",
    "    data['EMV'] = TA.EMV(ohlcv)\n",
    "    data['CCI'] = TA.CCI(ohlcv)\n",
    "    data['FISH'] = TA.FISH(ohlcv)\n",
    "    data['FVE'] = TA.FVE(ohlcv)\n",
    "\n",
    "    macd = TA.MACD(ohlcv)\n",
    "    data['MACDCal'] = macd['MACD'] - macd['SIGNAL']\n",
    "\n",
    "    macdev = TA.EV_MACD(ohlcv)\n",
    "    data['EVMACD'] = macdev[\"MACD\"]\n",
    "\n",
    "    data['TR'] = TA.TR(ohlcv)\n",
    "\n",
    "    DMI = TA.DMI(ohlcv)\n",
    "    data['DMI+'] = DMI[\"DI+\"]\n",
    "    data['DMI-'] = DMI[\"DI-\"]\n",
    "\n",
    "    VORTEX = TA.VORTEX(ohlcv)\n",
    "    data['VIp'] = VORTEX[\"VIp\"]\n",
    "    data['ADL'] = TA.ADL(data)\n",
    "\n",
    "    TSI = TA.TSI(ohlcv)\n",
    "    data['TSI'] = TSI[\"TSI\"]\n",
    "    data['TSIsignal'] = TSI[\"signal\"]\n",
    "\n",
    "    KST = TA.KST(ohlcv)\n",
    "    data['KST'] = KST[\"KST\"]\n",
    "\n",
    "    data['CHAIKIN'] = TA.CHAIKIN(ohlcv)\n",
    "    data['OBV'] = TA.OBV(ohlcv)\n",
    "    data['WOBV'] = TA.WOBV(ohlcv)\n",
    "\n",
    "    EBBP = TA.EBBP(ohlcv)\n",
    "    data['EBBPBull'] = EBBP[\"Bull.\"]\n",
    "    data['EBBPBear'] = EBBP[\"Bear.\"]\n",
    "\n",
    "    BASPN = TA.BASPN(ohlcv)\n",
    "    data['BASPNBuy'] = BASPN[\"Buy.\"]\n",
    "    data['BASPNSell'] = BASPN[\"Sell.\"]\n",
    "    data['COPP'] = TA.COPP(ohlcv)\n",
    "\n",
    "    BASP = TA.BASP(ohlcv)\n",
    "    data['BASPBuy'] = BASP[\"Buy.\"]\n",
    "    data['BASPSell'] = BASP[\"Sell.\"]\n",
    "\n",
    "    WTO = TA.WTO(ohlcv)\n",
    "    data['WTOWT1'] = WTO[\"WT1.\"]\n",
    "\n",
    "    data['STC'] = TA.STC(ohlcv)\n",
    "    data['VPT'] = TA.VPT(ohlcv)\n",
    "    return data\n",
    "\n",
    "LOOK_BACK = 10\n",
    "\"\"\"Hàm này dùng để phân chia data ra theo từng ngày nối đuôi nhau vựa trên lock_back\"\"\"\n",
    "# Xem xem trung bình 10 ngày tăng hay giảm, là dùng để xem xu hướng của tiền\n",
    "def create_dataset(data, label, look_back=1):  #\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    # Tạo vòng lặp để lấy các lookback\n",
    "    for i in range(len(data)-look_back-1):\n",
    "        X_.append(data[i:(i+look_back)])\n",
    "        y_.append(label[i + look_back])\n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "\"\"\" Hàm này để lấy các ngày liên tiếp theo n_days để làm các cột dữ liệu\"\"\"\n",
    "def add_past_days_as_feature(data: pd.DataFrame, n_days: int = 5):\n",
    "    data = pd.concat([data.shift(i).add_suffix(f\"_{i}\") for i in range(n_days)], axis=1)\n",
    "    return data\n",
    "\n",
    "\" Hàm Bias dùng để so sánh giữ hai hai tính hiệu dài hạn và ngắn hạn vựa trên short_val và long_val \"\n",
    "def bias(prices):\n",
    "    short_avg = prices['Close'].rolling(3, min_periods=1).mean()\n",
    "    long_avg = prices['Close'].rolling(5, min_periods=1).mean()\n",
    "\n",
    "    short_val = pd.Series(((prices['Close'] - short_avg) / short_avg) * 100, name=\"BIAS_short\", index=prices.index)\n",
    "    long_val = pd.Series(((prices['Close'] - long_avg) / long_avg) * 100, name=\"BIAS_long\", index=prices.index)\n",
    "    indi = pd.concat([short_val, long_val], axis=1)\n",
    "\n",
    "    # So sánh xem dài hạn hay ngắn hạn cái nào sẽ lời hơn\n",
    "    signal = pd.Series((long_val > short_val).astype(int), name=\"BIAS_signal\", index=prices.index)\n",
    "    return indi, signal\n",
    "\n",
    "def vr(prices):\n",
    "    maximum = (prices['High'] + prices['Close'].shift(1).bfill()).mean() # Lấy giá cao nhất ngày hiện tạo cộng cho ngày đóng cửa của tương lai,\n",
    "    # Nếu tương lai là Nan thì cộng cho giá đóng cưa hiện tại\n",
    "    minimum = (prices['Low'] + prices['Close'].shift(1).bfill()).mean()\n",
    "    high = prices['High'].rolling(14, min_periods=1).mean()\n",
    "    low = prices['Low'].rolling(14, min_periods=1).mean()\n",
    "\n",
    "    # Tính chỉ số\n",
    "    indi = pd.Series((maximum - minimum) / (high - low), name=\"VR\", index=prices.index)\n",
    "    signal = pd.Series((indi > 0.5).astype(int), name=\"VR_signal\", index=prices.index)\n",
    "    return indi, signal\n",
    "\n",
    "# Hàm tính giá trị Trix\n",
    "def trix(prices):\n",
    "    indi = TA.TRIX(prices, 10)\n",
    "    signal = pd.Series((indi < 0).astype(int), name=\"TRIX_signal\", index=prices.index)\n",
    "    return indi, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_features(data_new):\n",
    "    price = {}\n",
    "    # Lấy data của giá đóng cửa của loại đồng tiền\n",
    "    for symbol in data_new.keys():\n",
    "        price[symbol] = data_new[symbol]['Close'].copy()\n",
    "    y = {}\n",
    "    # y ở đây là ngày mai tăng hay giảm\n",
    "    # Y lấy dữ liệu giá đóng cửa. Nếu giá trị đang Nan thì điền vào giá trị gần nhất của cột Close\n",
    "    for symbol in data_new.keys():\n",
    "        #y[symbol] = data_new[symbol][\"Close\"].shift(-1).ffill()\n",
    "        y[symbol] = data_new[symbol][\"Close\"].ffill()\n",
    "    # x là features\n",
    "    X = {}\n",
    "    # Đoạn này lấy dữ liệu X là data dùng để tách train, test\n",
    "    for symbol in data_new.keys():\n",
    "        X_base_features = create_indicators(data_new[symbol])\n",
    "        X[symbol] = add_past_days_as_feature(data=X_base_features, n_days=LOOK_BACK)\n",
    "        X[symbol][\"label\"] = y[symbol].copy()\n",
    "        X[symbol] = X[symbol].dropna(axis=0)\n",
    "        y[symbol] = X[symbol][\"label\"].copy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_scale(X: dict, y: dict, symbols: dict):\n",
    "    data_all_money = {}\n",
    "    for symbol in symbols[\"all\"]:\n",
    "        # Lấy dữ liệu gốc khi (chưa scale)\n",
    "        data_original = X[symbol].copy()\n",
    "        y_ = pd.Series(y[symbol])\n",
    "        X_train = data_original.iloc[:-1]\n",
    "        y_train =y_.iloc[:-1]\n",
    "        X_test = data_original.iloc[-1]\n",
    "        y_test =y_.iloc[-1]\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        data_all_money[symbol] = [X_train_scaled, X_test, y_train, y_test]\n",
    "    return data_all_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ta đi smooth data để smooth ta cần biết các dữ liệu nhiễu thường xuyên, và bộ dữ liệu thực tế\n",
    "# Việc cần ở đây là chỉ số nhiễu tượng trưng cho dữ liệu đó chỉ cần chỉnh bao nhiêu phần trăm bị nhiễu\n",
    "def Smoothing_data(y, symbols) -> dict:\n",
    "  symbolsmooths = {}\n",
    "  # cho nhiều bộ smooths y khác nhau để xem cái nào tốt nhất\n",
    "  for syms in symbols[\"all\"]:\n",
    "    smooths = []\n",
    "    for isf in range(4,5):\n",
    "        for ins in range(4,5):\n",
    "            smoothing_factor = isf\n",
    "            n_seasons = ins\n",
    "            # --- define state transition matrix A tạo ma trận n_season+1\n",
    "            state_transition = np.zeros((n_seasons+1, n_seasons+1))\n",
    "            # hidden level         Đặt mặc định số đầu tiên là 1\n",
    "            state_transition[0,0] = 1\n",
    "            # season cycle         Sau đó ta chỉnh các thông sô khác cho mặc định dòng 1 có n_season = -1\n",
    "            state_transition[1,1:-1] = [-1.0] * (n_seasons-1)\n",
    "            # Tạo đường chéo chính có giá trị là 1\n",
    "            state_transition[2:,1:-1] = np.eye(n_seasons-1)\n",
    "            # --- observation model H\n",
    "            # observation is hidden level + weekly seasonal compoenent\n",
    "            observation_model = [[1,1] + [0]*(n_seasons-1)]\n",
    "            # --- noise models, parametrized by the smoothing factor\n",
    "            level_noise = 0.2 / smoothing_factor\n",
    "            observation_noise = 0.2\n",
    "            season_noise = 1e-3\n",
    "            process_noise_cov = np.diag([level_noise, season_noise] + [0]*(n_seasons-1))**2\n",
    "            observation_noise_cov = observation_noise**2\n",
    "            # Sử dụng Kalman filter\n",
    "            kf = simdkalman.KalmanFilter(\n",
    "                state_transition, # Hiển thị cách model chuyển tiếp để thay đổi ( Đây là cách nhìn dữ liệu bị nhiễu)\n",
    "                process_noise_cov, # ma trận biểu thị mức độ nhiễu của mô hình (Đây là để biết nhiễu thường xảy ra với tuần suât nào)\n",
    "                observation_model, #  Là mô hình quan sát của hệ thông thường là ma trận hay hàm số biểu thị (Đây là  kết quả hiện tại đang cần chỉnh nhiễu)\n",
    "                observation_noise_cov #  Ma trận biểu thị mức độ nhiễu trong đo lường  (Đây giống như là sai số để xem kết quả dự đoán với thực tế thì sau bnhieu)\n",
    "                )\n",
    "            # Sau đó ta tính toán lấy ra được các dự đoán nhiễu và trã về dự đoán bị nhiễu được quy định bằng các tham số trên\n",
    "            block = y[syms]\n",
    "            n_train = block.shape[0]\n",
    "            n_test = 60\n",
    "            result = kf.compute(block, n_test)\n",
    "            predictproba = result.smoothed.states.mean[:,0]\n",
    "            y_label = []\n",
    "            for ivalue in range(1,len(predictproba)):\n",
    "                if(predictproba[ivalue] > predictproba[ivalue-1]):\n",
    "                    y_label.append(1)\n",
    "                else:\n",
    "                    y_label.append(-1)\n",
    "            smooths.append(y_label)\n",
    "    symbolsmooths[syms] = smooths\n",
    "  return symbolsmooths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_label(y_train_ ,symbolsmooths, syms ):\n",
    "  # Hàm chỉnh dữ liệu lại cho smooth\n",
    "  y_train = []\n",
    "  y_test = []\n",
    "  # chạy từng mã gán lại nhãn đã smooth, -1 là 0 giảm, 1 là 1 tăng\n",
    "  for idx in range(len(y_train_)):\n",
    "      if symbolsmooths[syms][0][idx]  == -1:\n",
    "          y_train.append(0)\n",
    "      else:\n",
    "          y_train.append(1)\n",
    "  # y_test cũng vậy, phần test sẽ là phần còn lại tính từ n_split\n",
    "  for idx in range(len(symbolsmooths[syms][0]) - len(y_train_)):\n",
    "      if symbolsmooths[syms][0][idx+ len(y_train_)] == -1:\n",
    "          y_test.append(0)\n",
    "      else:\n",
    "          y_test.append(1)\n",
    "  y_test.append(1)\n",
    "  return y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_all_data_forex(X, y, symbols):\n",
    "  data_all = {}\n",
    "  data_all_money = split_data_scale(X,y, symbols)\n",
    "  symbolsmooth_y = Smoothing_data(y, symbols)\n",
    "\n",
    "  for sym in symbols[\"all\"]:\n",
    "    # Lấy các cột của dữ liệu\n",
    "    df = pd.DataFrame(X[sym] )\n",
    "    col_df = df.columns\n",
    "\n",
    "    # Lấy đúng loại tiền cần dùng\n",
    "    X_train, X_test, y_train, y_test = data_all_money[sym]\n",
    "    # Sau khi smooth data ta có y_train và y_test mới\n",
    "    y_train, y_test = smooth_label( y_train, symbolsmooth_y, sym)\n",
    "    # Sau đó  tạo ra các dataframe mới\n",
    "    X_train = pd.DataFrame(X_train, columns = col_df)\n",
    "    X_test =  X_test.to_frame().T\n",
    "    # Drop các cột không cần thiết là label đi\n",
    "    X_train = X_train.drop(columns = [\"label\"])\n",
    "    X_test = X_test.drop(columns = [\"label\"])\n",
    "    # Lấy dữ liệu tiền tệ đó ra\n",
    "    data_set = pd.get_dummies(X_train, drop_first=False)\n",
    "    data_all[sym] = [data_set, X_test, y_train, y_test]\n",
    "  return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm khởi tạo Isolation forest cho việc loại bỏ các outlier vựa trên decision Tree\n",
    "# Nó chỉ loại bỏ 1 phần bị outliers\n",
    "def create_clf(X_train):\n",
    "  clf = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    n_jobs=-1,\n",
    "    random_state=2023)\n",
    "  clf.fit(X_train)\n",
    "  return clf\n",
    "# hàm tạo ngưỡng nhiễu\n",
    "def outlier_threshold(normality, k=1.5):\n",
    "  q1 = np.quantile(normality, 0.2)\n",
    "  q3 = np.quantile(normality, 0.8)\n",
    "  threshold = q1 - k*(q3-q1)\n",
    "  return threshold\n",
    "# Hàm loại bỏ outlier cho tập data train\n",
    "def delete_outlier(clf, X_train, y_train):\n",
    "  # Lấy ngưỡng cho dữ liệu\n",
    "  normality_df = pd.DataFrame(clf.decision_function(X_train), columns=['normality'])\n",
    "  threshold = outlier_threshold(normality_df['normality'].values, k=1.5)\n",
    "  # Sau đo ta loại bỏ dòng bị outlier\n",
    "  X_train = X_train[normality_df['normality'].values>=threshold]\n",
    "  y_train = y_train[normality_df['normality'].values>=threshold]\n",
    "  return X_train, y_train\n",
    "\n",
    "def data_np(data_all, symbols):\n",
    "  for sym in symbols[\"all\"]:\n",
    "    # Thay đổi dữ liệu về dạng làm tròn và numpy\n",
    "    data_all[sym][2] = np.array(data_all[sym][2])\n",
    "    data_all[sym][3] = np.array(data_all[sym][3])\n",
    "    data_all[sym][0] = data_all[sym][0].astype(np.float32)\n",
    "    data_all[sym][1] = data_all[sym][1].astype(np.float32)\n",
    "  return data_all\n",
    "\n",
    "def delete_outlier_data(data_all, symbols):\n",
    "  data_all = data_np(data_all, symbols)\n",
    "  for sym in symbols[\"all\"]:\n",
    "    clf = create_clf(data_all[sym][0])\n",
    "    data_all[sym][0], data_all[sym][2] = delete_outlier(clf, data_all[sym][0], data_all[sym][2])\n",
    "  return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_symbols(symbols,key_symbols):\n",
    "    mmissing_symbols = []\n",
    "    for sym in symbols[\"all\"]:\n",
    "        if sym  not in key_symbols.keys():\n",
    "            mmissing_symbols.append(sym)\n",
    "    return mmissing_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_cv_for_miss_symbols(missing_symbols):\n",
    "  with open('best-cv-rf-and-xgb-and-mlp.pkl', \"rb\") as file:\n",
    "    best_cv_df_new= pickle.load(file)\n",
    "    best_cv_df ={}\n",
    "  for sym in missing_symbols:\n",
    "    sorted_df = best_cv_df_new[sym].copy().sort_values(by=['model', 'accuracy'], ascending=[True, False])\n",
    "    best_cv_df[sym] = sorted_df\n",
    "  for sym in missing_symbols:\n",
    "    best_cv_df[sym] = pd.DataFrame(best_cv_df[sym])\n",
    "  return best_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm này để đánh giá mô hình tổng\n",
    "# Là khi chúng ta train ra hết 3 model chúng ta xem rằng chúng kết hợp với nhau thì kết quả cuối cùng độ chính xác là bao nhiêu\n",
    "def CV_ensemble(ensemble_name, ensemble_func, estimators, X_train, y_train, n_folds=5, shuffle=True, random_state=2023):\n",
    "  kf = KFold(n_splits=5, random_state=random_state, shuffle=True) # Tạo số lượng dữ liệu bằng Kfold sau đó shuffle\n",
    "\n",
    "  res_list = [] # tạo res_list lưu thông tin của từng lần chia kfold\n",
    "  # Chạy tập dữ liệu Kfold\n",
    "  for train_idx, valid_idx in notebook.tqdm(kf.split(X_train), total=kf.get_n_splits(), desc='Eval_CV'): # Chạy theo số lần n_splits\n",
    "    # print(len(X_train))\n",
    "    # print(len(y_train))\n",
    "    X_train_train, X_valid = X_train[train_idx], X_train[valid_idx]\n",
    "    y_train_train, y_valid = y_train[train_idx], y_train[valid_idx]\n",
    "    # Tổng hợp kết quả test đối vơi từng cặp k riêng\n",
    "    ensemble_pred_proba = ensemble_func(estimators, X_train_train, y_train_train, X_valid)\n",
    "    # Lấy các đánh giá  của model với tập train test k này\n",
    "    neg_log_loss = np.negative(log_loss(y_valid, ensemble_pred_proba))\n",
    "    accuracy = accuracy_score(y_valid, ensemble_pred_proba.argmax(axis=1))\n",
    "\n",
    "    res_list.append([ensemble_name, neg_log_loss, accuracy]) # Lưu kết quả lại`\n",
    "  res_df = pd.DataFrame(np.vstack((res_list)))\n",
    "  res_df.columns = ['model', 'log_loss', 'accuracy']  # thêm các thông số để đánh giá\n",
    "  return res_df\n",
    "\n",
    "# hàm này để dự đoán bằng cách tổng hợp 3 model có train lại\n",
    "def ensemble_average(estimators, X_train, y_train, X_test):\n",
    "  # Hàm này vựa trên việc lấy xác suất của các model vote cái nào có xác suất cao nhất thì chọn\n",
    "  preds = []\n",
    "  num_estimators = len(estimators)\n",
    "  num_class = len(np.unique(y_train))\n",
    "  for iter in range(num_estimators):\n",
    "    y_train = np.array(y_train, dtype=np.int64)\n",
    "    estimators[iter].fit(X_train, y_train)\n",
    "    preds.append(estimators[iter].predict_proba(X_test))\n",
    "\n",
    "  preds_stack = np.hstack((preds))\n",
    "  preds_mean = []\n",
    "  for iter in range(num_class):\n",
    "    col_idx = np.arange(iter, num_estimators * num_class, num_class)\n",
    "    preds_mean.append(np.mean(preds_stack[:,col_idx], axis=1))\n",
    "\n",
    "  avg_pred = np.vstack((preds_mean)).transpose()\n",
    "  return avg_pred\n",
    "\n",
    "# hàm này đưa ra dự đoán mà ko cần train lại\n",
    "def ensemble_average_model(estimators, X_test,y_train): # esitamtors là các model và X_test là dữ liệu train\n",
    "  preds = []\n",
    "  num_estimators = len(estimators)\n",
    "  num_class = len(np.unique(y_train))\n",
    "  # đoạn này lấy tổng dữ đoán của các model trong estimators\n",
    "  for iter in range(num_estimators):\n",
    "    preds.append(estimators[iter].predict_proba(X_test))\n",
    "  preds_stack = np.hstack((preds))\n",
    "  preds_mean = []\n",
    "  # Dự đoán trung bình cho mỗi nhãn\n",
    "  for iter in range(num_class):\n",
    "    col_idx = np.arange(iter, num_estimators * num_class, num_class) # Tính cho số cột cho nhãn được dự đoán\n",
    "    preds_mean.append(np.mean(preds_stack[:,col_idx], axis=1)) # Tính giá trị trung bình cho dự đoán\n",
    "\n",
    "  avg_pred = np.vstack((preds_mean)).transpose()# Lấy hết các giá trị tring bình sau đó chuyển vị\n",
    "  return avg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_all,missing_symbols,best_cv_df):\n",
    "      res_df_all = {}\n",
    "      estimators_all_sym = {}\n",
    "      for sym in missing_symbols:\n",
    "      # Lưu trữ sym vét cạn\n",
    "            estimators_all ={}\n",
    "            X_train = data_all[sym][0]\n",
    "            y_train = data_all[sym][2]\n",
    "            X = np.array(X_train)\n",
    "            y = np.array(y_train, dtype=np.int64)\n",
    "            # load 3 cấu hình tốt nhất cho 3 model vào biến rf xbg và mlp\n",
    "            rf1 = RandomForestClassifier(**eval(best_cv_df[sym].loc[best_cv_df[sym]['model']=='rf', 'best_hyper_param'].values[0]))\n",
    "            xgb1 = XGBClassifier(**eval(best_cv_df[sym].loc[best_cv_df[sym]['model']=='xgb', 'best_hyper_param'].values[0]))\n",
    "            mlp1 = MLPClassifier(**eval(best_cv_df[sym].loc[best_cv_df[sym]['model']=='mlp', 'best_hyper_param'].values[0]))\n",
    "            # quá trình tổng hopkw 3 model để tạo ra model trung bình tốt nhất\n",
    "            estimators = [rf1, xgb1, mlp1]\n",
    "            estimators_name = 'rf_xgb_mlp'\n",
    "            ensemble_name = 'average' + '_by_' + estimators_name + str(0) + str(0) + str(0)\n",
    "            # Lấy res_df ra\n",
    "            res_df = CV_ensemble(ensemble_name, ensemble_average, estimators, X, y, n_folds=5, shuffle=True, random_state=2023)\n",
    "            sym_brute_force = sym + str(0) + str(0) + str(0)\n",
    "            res_df_all[sym_brute_force] = res_df\n",
    "\n",
    "            estimators_all[sym_brute_force] = estimators\n",
    "            print(sym_brute_force)\n",
    "            print(res_df)\n",
    "            estimators_all_sym[sym] = estimators_all\n",
    "      return estimators_all_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(best_model_tuning,best_model_missing):\n",
    "    for sym in best_model_missing.keys():\n",
    "        best_model_tuning[sym] =  best_model_missing[sym]\n",
    "    file_name = 'best-cv-rf-and-xgb-and-mlp.pkl.pkl'\n",
    "    # Lưu từ điển vào tệp pickle\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(best_model_tuning, file)\n",
    "    return best_model_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_test_y_train(data_all,symbols):\n",
    "    X_test = {}\n",
    "    for symbol in symbols['all']:\n",
    "        X_test[symbol] = data_all[symbol][1].copy()\n",
    "    y_train = data_all[symbol][2]\n",
    "    return X_test,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sym(estimators_sym,key_symbols,X_test,y_train):\n",
    "  pred_y_label = {}\n",
    "  for sym in key_symbols:\n",
    "    estimators = estimators_sym[sym]\n",
    "    pred_y = ensemble_average_model(estimators,X_test,y_train)\n",
    "    pred_y = pred_y.argmax(axis=1)\n",
    "    pred_y_label[sym] = pred_y\n",
    "  return pred_y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_sym(estimators_all_sym,data_all):\n",
    "  symbols = get_symbol(estimators_all_sym)\n",
    "  key_symbols = get_keysymbol(estimators_all_sym)\n",
    "  X_test,y_train = get_X_test_y_train(data_all,symbols)\n",
    "  pred_y_label_all ={}\n",
    "  for sym in symbols['all']:\n",
    "    pred_y_label = predict_sym(estimators_all_sym[sym],key_symbols[sym],X_test[sym],y_train)\n",
    "    pred_y_label_all[sym] = pred_y_label\n",
    "  return pred_y_label_all  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_output(data):\n",
    "  simplified = {}\n",
    "  # Duyệt qua từng cặp tiền tệ\n",
    "  for pair, signals in data.items(): \n",
    "\n",
    "    # Đếm tín hiệu mua/bán\n",
    "    signal_counts = {'buy': 0, 'sell': 0}\n",
    "    \n",
    "    for name, signal in signals.items():\n",
    "      if signal[0] == 1:\n",
    "        signal_counts['buy'] += 1  \n",
    "      else:\n",
    "        signal_counts['sell'] += 1\n",
    "\n",
    "    # Nếu toàn bộ là mua hoặc bán\n",
    "    if signal_counts['buy'] == 0 or signal_counts['sell'] == 0:  \n",
    "      simplified[pair] = 1 if signal_counts['buy'] > 0 else 0\n",
    "    # Nếu có cả mua và bán    \n",
    "    else:\n",
    "      if signal_counts['buy'] > signal_counts['sell']:\n",
    "        simplified[pair] = 1\n",
    "      else: \n",
    "        simplified[pair] = 0\n",
    "        \n",
    "  return simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal(symbols):\n",
    "    best_model_tuning,key_symbols = load_sym_model()\n",
    "    missing_symbols = check_missing_symbols(symbols,key_symbols)\n",
    "    data_new = get_data(symbols)\n",
    "    X,y = set_up_features(data_new)\n",
    "    data_all = create_train_test_all_data_forex(X,y,symbols)\n",
    "    data_clear = delete_outlier_data(data_all,symbols)\n",
    "    best_cv_df = get_best_cv_for_miss_symbols(missing_symbols)\n",
    "    # best_model_missing =  train_model(data_all,missing_symbols,best_cv_df)\n",
    "    # merged_model = save_model(best_model_tuning,best_model_missing)\n",
    "    pred_y_label_all = predict_all_sym(merged_model,data_clear)\n",
    "    pred_all = simplify_output(pred_y_label_all)\n",
    "    return pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = {'all': ['EURJPY']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURJPY\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc6b5e98bc54fb1a1159701beb25674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval_CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURJPY000\n",
      "                      model              log_loss            accuracy\n",
      "0  average_by_rf_xgb_mlp000    -0.454155299720914  0.8289473684210527\n",
      "1  average_by_rf_xgb_mlp000  -0.46108373625358334   0.819078947368421\n",
      "2  average_by_rf_xgb_mlp000   -0.4687722926451905  0.7894736842105263\n",
      "3  average_by_rf_xgb_mlp000  -0.43896570302871096   0.805921052631579\n",
      "4  average_by_rf_xgb_mlp000  -0.45264471690567254  0.8283828382838284\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'EURCHF'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_y_label_all \u001b[38;5;241m=\u001b[39m get_signal(symbols)\n",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mget_signal\u001b[1;34m(symbols)\u001b[0m\n\u001b[0;32m      9\u001b[0m best_model_missing \u001b[38;5;241m=\u001b[39m  train_model(data_all,missing_symbols,best_cv_df)\n\u001b[0;32m     10\u001b[0m merged_model \u001b[38;5;241m=\u001b[39m save_model(best_model_tuning,best_model_missing)\n\u001b[1;32m---> 11\u001b[0m pred_y_label_all \u001b[38;5;241m=\u001b[39m predict_all_sym(merged_model,data_clear)\n\u001b[0;32m     12\u001b[0m pred_all \u001b[38;5;241m=\u001b[39m simplify_output(pred_y_label_all)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_all\n",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m, in \u001b[0;36mpredict_all_sym\u001b[1;34m(estimators_all_sym, data_all)\u001b[0m\n\u001b[0;32m      2\u001b[0m symbols \u001b[38;5;241m=\u001b[39m get_symbol(estimators_all_sym)\n\u001b[0;32m      3\u001b[0m key_symbols \u001b[38;5;241m=\u001b[39m get_keysymbol(estimators_all_sym)\n\u001b[1;32m----> 4\u001b[0m X_test,y_train \u001b[38;5;241m=\u001b[39m get_X_test_y_train(data_all,symbols)\n\u001b[0;32m      5\u001b[0m pred_y_label_all \u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sym \u001b[38;5;129;01min\u001b[39;00m symbols[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m, in \u001b[0;36mget_X_test_y_train\u001b[1;34m(data_all, symbols)\u001b[0m\n\u001b[0;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m symbols[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     X_test[symbol] \u001b[38;5;241m=\u001b[39m data_all[symbol][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      5\u001b[0m y_train \u001b[38;5;241m=\u001b[39m data_all[symbol][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_test,y_train\n",
      "\u001b[1;31mKeyError\u001b[0m: 'EURCHF'"
     ]
    }
   ],
   "source": [
    "pred_y_label_all = get_signal(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EURJPY000': [RandomForestClassifier(criterion='entropy', max_depth=8, n_estimators=300),\n",
       "  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=0.5, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "                min_child_weight=5, missing=nan, monotone_constraints=None,\n",
       "                multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "                num_parallel_tree=None, random_state=2023, ...),\n",
       "  MLPClassifier(activation='tanh', alpha=0.1, hidden_layer_sizes=(120, 80, 40),\n",
       "                max_iter=80, solver='sgd')]}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'best_model_tuning.pkl'\n",
    "# # Lưu từ điển vào tệp pickle\n",
    "# with open(file_name, 'wb') as file:\n",
    "#     pickle.dump(pred_y_label_all, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
